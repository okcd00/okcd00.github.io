<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
	
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="baidu-site-verification" content="o4pXUBsPyW" />
	
    <!--Description-->
    
        <meta name="description" content="To be more curious and less lazy.">
    

    <!--Author-->
    
        <meta name="author" content="Chen Dian">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="【Autograd】深入理解BP与自动求导"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="To be more curious and less lazy." />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="CDPlayer&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>【Autograd】深入理解BP与自动求导 - CDPlayer&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Google Analytics -->
    

	
	<!-- Baidu Analytics -->
    
	<script type="text/javascript">
		var _hmt = _hmt || [];
		(function() {
		  var hm = document.createElement("script");
		  hm.src = "https://hm.baidu.com/hm.js?{{ theme.baidu_analytics }}";
		  var s = document.getElementsByTagName("script")[0];
		  s.parentNode.insertBefore(hm, s);
		})();
	</script>


	
	<!-- star map -->
	<script type="text/javascript" color="255,255,255" opacity='0.75' zIndex="-2" count="88" src="js/canvas.js"></script>
	
	<!-- Earth -->
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mihyim0z76&amp;m=7&amp;c=66ccff&amp;cr1=ff0000&amp;f=arial&amp;l=0&amp;lx=200&amp;ly=-20&amp;hi=10" async="async"></script>

<meta name="generator" content="Hexo 5.4.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/articles/2017-10/%E3%80%90Autograd%E3%80%91%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3BP%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC.html">
                【Autograd】深入理解BP与自动求导
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-10-18</span>
            
            
                <a href="#disqus_thread" class="comments">留言</a>
            
            
                <span class="category">
                    <a href="/categories/机器学习/">机器学习</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

		<!-- MathJax support -->
		
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
	tex2jax: {
	  inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
	  processEscapes: true,
	  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
	}
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
	var all = MathJax.Hub.getAllJax(), i;
	for (i=0; i < all.length; i += 1) {
	  all[i].SourceElement().parentNode.className += ' has-jax';
	}
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>



		
        <!-- Post Content -->
        <h3 id=""><a href="#" class="headerlink" title=""></a></h3><blockquote>
<p><strong>“所有数值计算归根结底是一系列有限的可微算子的组合”</strong><br>——《<a target="_blank" rel="noopener" href="http://tejas.serc.iisc.ernet.in/currsci/apr102000/tutorial1.pdf">An introduction to automatic differentiation</a>》 </p>
</blockquote>
<span id="more"></span>

<h3 id="符号语言的导数"><a href="#符号语言的导数" class="headerlink" title="符号语言的导数"></a>符号语言的导数</h3><blockquote>
<p>《Deep Learning》 Chap 6.5.5</p>
</blockquote>
<p>代数表达式和计算图都对符号(symbol) 或不具有特定值的变量进行操作。这些代数或者基于图的表达式被称为符号表示(symbolic representation)。<br>当我们实际使用或者训练神经网络时，我们必须给这些符号赋值。我们用一个特定的数值(numeric value) 来替代网络的符号输入x，例如 $[1.2, 3, 765, -1.8]^T$。</p>
<p><img src="/assets/1508315592269.png" alt="Alt text"></p>
<p><strong>符号到数值</strong>的微分<br>一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值。我们将这种方法称为‘‘符号到数值’’ 的微分。这种方法用在诸如<strong>Torch</strong>(Collobert et al., 2011b)和<strong>Caffe</strong>(Jia, 2013)之类的库中。  </p>
<p><strong>符号到符号</strong>的微分<br>另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。这是<strong>Theano</strong>(Bergstra et al., 2010b; Bastien et al., 2012b) 和<strong>TensorFlow</strong>(Abadi et al., 2015) 采用的方法。图6.10 中给出了该方法如何工作的一个例子。这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。</p>
<p><img src="/assets/1508315628313.png" alt="Alt text"></p>
<p><strong>TensorFlow</strong>中实现的自动求导（automatic gradient / Automatic Differentiation）：<br>实现的方式是利用<strong>反向传递</strong>与<strong>链式法则</strong>建立一张对应原计算图的梯度图。因为导数只是另外一张计算图，可以再次运行反向传播，对导数再进行求导以得到更高阶的导数。（这里我们重点讲这一种，所以下面几个小节会对反向传递算法与链式法则作简要概述）</p>
<p><img src="/assets/1508394713356.png" alt="Alt text"></p>
<h3 id="Backpropagation-algorithm"><a href="#Backpropagation-algorithm" class="headerlink" title="Backpropagation algorithm"></a>Backpropagation algorithm</h3><blockquote>
<p>【<strong>Reference</strong>】 <a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)"><strong>Proof</strong> of the fundamental equations (optional for reading)</a></li>
<li><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html#exercises_675621"><strong>Another version</strong> of Back-prop for mini-batch</a></li>
</ul>
</blockquote>
<ol>
<li><strong>Input</strong> $x$ : Set the corresponding activation $a^1$ for the input layer.</li>
<li><strong>Feedforward</strong> : For each $l=2,3,…,L$ compute $z^l = w^la^{l-1}+b^l$ and $a^l = \sigma (z^l)$</li>
<li><strong>Output error</strong> $\delta^L$ : Compute the vector $\delta^L = \nabla_aC \bigodot \sigma^\prime(z^L)$</li>
<li><strong>Backpropagate the error</strong> : For each $l=L-1,L-2,…,2$ compute $\delta^l = ((w^{l+1})^{\top} \delta^{l+1}) \bigodot \sigma^\prime(z^l)$</li>
<li><strong>Output</strong> : The gradient of the cost function is given by $\frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l$ and $\frac{\partial C}{\partial b_{j}^l} = \delta_j^l$</li>
</ol>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html#warm_up_a_fast_matrix-based_approach_to_computing_the_output_from_a_neural_network">算法中用到的符号解释</a></p>
</blockquote>
<ul>
<li>We’ll use $w_{jk}^l$ to denote the weight for the connection from the kthkth neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.</li>
<li>We use $b_j^l$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a_j^l$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer. </li>
<li>We use $s \bigodot t$ to denote the elementwise product of the two vectors. Thus the components of $s \bigodot t$ are just $(s \bigodot t)_j=s_j t_j$ </li>
</ul>
<h3 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h3><blockquote>
<p><a target="_blank" rel="noopener" href="http://dlsys.cs.washington.edu/schedule">CSE599G1</a>: Deep Learning System (陈天奇)</p>
</blockquote>
<p>微分求解大致可以分为4种方式：</p>
<ul>
<li><p>手动求解法(Manual Differentiation)</p>
<ul>
<li>求解出梯度公式，然后编写代码，代入实际数值，得出真实的梯度。在这样的方式下，每一次我们修改算法模型，都要修改对应的梯度求解算法，因此没有很好的办法解脱用户手动编写梯度求解的代码。</li>
</ul>
</li>
<li><p>数值微分法(Numerical Differentiation)</p>
<ul>
<li>不能完全消除truncation error，只是将误差减小。但是由于它实在是太简单实现了，于是很多时候，我们利用它来检验其他算法的正确性，比如在实现backprop的时候，我们用的”gradient check”就是利用数值微分法。</li>
</ul>
</li>
<li><p>符号微分法(Symbolic Differentiation)</p>
<ul>
<li>“表达式膨胀”（expression swell）问题，如果不加小心就会使得问题符号微分求解的表达式急速“膨胀”，导致最终求解速度变慢，如本小节末的图表<code>Table 1</code>所示。</li>
<li>Implement differentiation rules:<ul>
<li>Product Rule: $\frac{d(f+g)}{dx}=\frac{df}{dx}+\frac{dg}{dx}$</li>
<li>Sum Rule: $\frac{d(fg)}{dx}=\frac{df}{dx}g+f\frac{dg}{dx}$</li>
<li>Chain Rule: $\frac{d(h(x))}{dx}=\frac{df(g(x))}{dx}+\frac{dg(x)}{x}$</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>自动微分法</strong>(Automatic Differentiation)</p>
<ul>
<li>自动微分法是一种介于符号微分和数值微分的方法：数值微分强调一开始直接代入数值近似求解；符号微分强调直接对代数进行求解，最后才代入问题数值；自动微分将符号微分法应用于最基本的算子，比如常数，幂函数，指数函数，对数函数，三角函数等，然后代入数值，保留中间结果，最后再应用于整个函数。因此它应用相当灵活，可以做到完全向用户隐藏微分求解过程，由于它<strong>只对基本函数或常数</strong>运用符号微分法则，所以它可以<strong>灵活结合</strong>编程语言的循环结构，条件结构等，使用自动微分和不使用自动微分对代码总体改动非常小，并且由于它的计算实际是一种<strong>图计算</strong>，可以对其做很多优化，这也是为什么该方法在现代深度学习系统中得以广泛应用。</li>
</ul>
</li>
</ul>
<p><img src="/assets/1508406083327.png" alt="Alt text"></p>
<h3 id="Backpropagation-vs-AutoDiff-reverse"><a href="#Backpropagation-vs-AutoDiff-reverse" class="headerlink" title="Backpropagation vs AutoDiff (reverse)"></a>Backpropagation vs AutoDiff (reverse)</h3><blockquote>
<p>CSE599G1 DeepLearning System Lecture4 —— [Slides View](/assets/CSE599G1 DeepLearning System Lecture4.pdf)</p>
</blockquote>
<ul>
<li>We can <strong>take derivative of derivative nodes</strong> in autodiff, while it’s much harder to do so in backprop.</li>
<li>In autodiff, there’s <strong>only a forward pass</strong> (vs. forward-backward in backprop). So it’s easier to apply graph and schedule optimization to a single graph.</li>
<li>In backprop, all intermediate results might be used in the future, so we need to keep these values in the memory. On the other hand, in autodiff, we already know the dependencies of the backward graph, so we can have <strong>better memory optimization</strong>.</li>
</ul>
<h3 id="Jacobi与链式法则"><a href="#Jacobi与链式法则" class="headerlink" title="Jacobi与链式法则"></a>Jacobi与链式法则</h3><blockquote>
<p>《Deep Learning》 Chap 6.5.2<br>该段引用了较多开源社区中对 Deep Learning 一书的中文翻译<br><a target="_blank" rel="noopener" href="https://github.com/exacity/deeplearningbook-chinese">https://github.com/exacity/deeplearningbook-chinese</a></p>
</blockquote>
<p>微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。<br>设 $x$ 是实数， $f$ 和 $g$ 是从实数映射到实数的函数。假设 $y = g(x)$ 并且 $z =f(g(x)) = f(y)$ 。那么链式法则是说</p>
<p>$$\frac{dz}{dx}=\frac{dz}{dy} \frac{dy}{dx}$$</p>
<p>我们可以将这种标量情况进行扩展。 假设$ x\in \mathbb{R}^m, y\in \mathbb{R}^n$，$g$是从$\mathbb{R}^m$到$\mathbb{R}^n$的映射，$f$是从$\mathbb{R}^n$到$\mathbb{R}$的映射。 如果$ y=g(x)$并且$z=f(y)$，那么<br>$$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}.$$</p>
<p>使用向量记法，可以等价地写成<br>$$\nabla_{x}z = \left ( \frac{\partial y}{\partial x} \right )^\top \nabla_{y} z,$$</p>
<p>通常我们不将反向传播算法仅用于向量，而是应用于任意维度的张量。从概念上讲，这与使用向量的反向传播完全相同。唯一的区别是如何将数字排列成网格以形成张量。<br>我们可以想象，在我们运行反向传播之前，将每个张量变平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。从这种重新排列的观点上看，反向传播仍然只是将 $Jacobi$ 矩阵乘以梯度。</p>
<p>如果 $Y=g(X)$ 并且 $z=f(Y)$，那么<br>$$\nabla_X z = \sum_j (\nabla_X Y_j)\frac{\partial z}{\partial Y_j}. $$</p>
<p>于是，反向传播算法就变得非常简单：<br>为了计算某个标量 $z$ 关于图中它的一个祖先 $x$ 的梯度，我们首先观察到它关于 $z$ 的梯度由 $\frac{dz}{dz}=1$ 给出。 然后，我们可以计算对图中 $z$ 的每个父节点的梯度，通过现有的梯度乘以产生$z$的操作的 $Jacobian$。 我们继续乘以 $Jacobian$，以这种方式向后穿过图，直到我们到达 $x$。 对于从 $z$ 出发可以经过两个或更多路径向后行进而到达的任意节点，我们简单地对该节点来自不同路径上的梯度进行求和。</p>
<p><img src="/assets/1508318743867.png" alt="Alt text"></p>
<h3 id="Tensorflow的自动求导实现"><a href="#Tensorflow的自动求导实现" class="headerlink" title="Tensorflow的自动求导实现"></a>Tensorflow的自动求导实现</h3><blockquote>
<p><strong>Tensorflow</strong> 中的符号求导见项目下的 tensorflow/python/ops/gradients_impl.py<br>“Constructs symbolic derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>”<br><code>[db, dW, dx] = tf.gradients(C, [b,W,x])</code></p>
</blockquote>
<p>《Deep Learning》一书中，表示<strong>Theano</strong>与<strong>Tensorflow</strong>采用如下图算法的子程序来建立 $grad_table$，而在<strong>Tensorflow</strong><a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf">白皮书</a>的第五节中，介绍了在$grad_table$中，存储了通常会被重复计算多次的 $ \partial u^{(n)} / \partial u^{(i)}$，用以减少程序的冗余计算从而增加效率：</p>
<blockquote>
<p>If a tensor $C$ in a TensorFlow graph depends, perhaps through a complex subgraph of operations, on some set of tensors $X_k$, then there is a built-in function that will return the tensors ${dC/dX_k}$.</p>
</blockquote>
<p><img src="/assets/1508319803477.png" alt="Alt text"></p>
<p>每个操作 $op$ 也与 $bprop$ 操作相关联。该 $bprop$ 操作可以计算如上述公式所描述的 $Jacobi$ 向量积。这是反向传播算法能够实现很大通用性的原因。每个操作负责了解如何通过它参与的图中的边来反向传播。反向传播算法本身并<strong>不需要</strong>知道任何微分法则。它只需要使用正确的参数调用每个操作的 $bprop$ 方法即可。正式地，$op.bprop(inputs,X,G)$ 必须返回</p>
<p>$$ \sum_i (\nabla_{X} \verb|op.f(inputs|)_i) \textsf{G}_i, $$</p>
<p>这里，$inputs$ 是提供给操作的一组输入，$op.f$ 是操作实现的数学函数，$X$ 是输入，我们想要计算关于它的梯度，$G$ 是操作对于输出的梯度。</p>
<blockquote>
<p>$op.bprop$ 方法应该总是假装它的所有输入彼此不同，即使它们不是。例如，如果 $mul$ 操作传递两个 $x$ 来计算 $x^2$，$op.bprop$ 方法应该仍然返回 $x$ 作为对于两个输入的导数。反向传播算法后面会将这些变量加起来获得 $2x$，这是 $x$ 上总的正确的导数。</p>
<p>反向传播算法的软件实现通常提供操作和其 $bprop$ 两种方法，所以深度学习软件库的用户能够对使用诸如矩阵乘法、指数运算、对数运算等等常用操作构建的图进行反向传播。构建反向传播新实现的软件工程师或者需要向现有库添加自己的操作的高级用户通常必须手动为新操作推导 $op.bprop$ 方法。</p>
</blockquote>
<p>我们以 $Tensorflow$ 的一次 <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/commit/7b7c02de56e013482b5fe5ab05e576dc98fe5742">commit</a>： <code>* Register log1p in math_ops.</code> 为例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 该文件为 tensorflow/core/ops/math_ops.cc</span></span><br><span class="line"><span class="comment">// 作用为注册操作log1p，定义为单元操作，以及提供说明文本</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">REGISTER_OP</span>(<span class="string">&quot;Log1p&quot;</span>)</span><br><span class="line">    .<span class="built_in">UNARY_COMPLEX</span>()</span><br><span class="line">    .<span class="built_in">Doc</span>(<span class="string">R&quot;doc(</span></span><br><span class="line"><span class="string">Computes natural logarithm of (1 + x) element-wise.</span></span><br><span class="line"><span class="string">I.e., \\(y = \log_e (1 + x)\\).</span></span><br><span class="line"><span class="string">)doc&quot;</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该文件为 tensorflow/python/ops/math_grad.py</span></span><br><span class="line"><span class="comment"># log1p的作用是求加一之后的自然对数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@ops.RegisterGradient(<span class="params"><span class="string">&quot;Log1p&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Log1pGrad</span>(<span class="params">op, grad</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Returns grad * (1/(1 + x)).&quot;&quot;&quot;</span></span><br><span class="line">  x = op.inputs[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">with</span> ops.control_dependencies([grad.op]):</span><br><span class="line">    x = math_ops.conj(x)</span><br><span class="line">    <span class="keyword">return</span> grad * math_ops.inv(<span class="number">1</span> + x)</span><br></pre></td></tr></table></figure>

<p>由于重复子表达式的存在，简单的算法可能具有指数运行时间。现在我们已经详细说明了反向传播算法，我们可以去理解它的计算成本：<br>对于与 $Theano$ 与 $Tensorflow$ 类似的平台，反向传播算法在原始图的每条边添加一个 $Jacobi$ 向量积，可以用 $O(1)$ 个节点来表达。因为计算图是有向无环图，它至多有 $O(n^2)$ 条边。<br>而对于实践中常用的图的类型，情况会更好：大多数神经网络的代价函数大致是链式结构的，使得反向传播只有 $O(n)$ 的成本。这远远胜过简单的方法，简单方法可能需要执行指数级的节点。这种潜在的指数级代价可以通过非递归地扩展和重写递归链式法则看出：</p>
<p>$$  \frac{\partial u^{(n)}}{\partial u^{(j)}} =<br>  \sum_{\substack{\text{path}(u^{(\pi_1)}, u^{(\pi_2)}, \ldots, u^{(\pi_t)}  ),\ \text{from } \pi_1=j \text{ to }\pi_t = n}}<br>  \prod_{k=2}^t \frac{\partial u^{(\pi_k)}}{\partial u^{(\pi_{k-1})}}. $$</p>
<p>由于节点 $j$ 到节点 $n$ 的路径数目可以关于这些路径的长度上指数地增长，所以上述求和符号中的项数（这些路径的数目），可能以前向传播图的深度的指数级增长。 会产生如此大的成本是因为对于 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$ ，相同的计算会重复进行很多次。为了避免这种重新计算，我们可以将反向传播看作一种表填充算法，利用存储的中间结果 $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ 来对表进行填充。 图中的每个节点对应着表中的一个位置，这个位置存储对该节点的梯度。 通过顺序填充这些表的条目，反向传播算法避免了重复计算许多公共子表达式——这种表填充策略有时被称为<strong>动态规划</strong>。</p>
<p><img src="/assets/1508418991710.png" alt="Alt text"></p>
<p>上述AutoDiff的图片来自于：<a target="_blank" rel="noopener" href="http://dlsys.cs.washington.edu/pdf/lecture4.pdf">http://dlsys.cs.washington.edu/pdf/lecture4.pdf</a></p>
<h3 id="高阶导数"><a href="#高阶导数" class="headerlink" title="高阶导数"></a>高阶导数</h3><p>一些软件框架支持使用高阶导数。在深度学习软件框架中，这至少包括Theano和TensorFlow。这些库使用一种数据结构来描述要被微分的原始函数，它们使用相同类型的数据结构来描述这个函数的导数表达式。这意味着符号微分机制可以应用于导数（从而产生高阶导数）。</p>
<blockquote>
<p>黑塞矩阵（Hessian Matrix），又译作海森矩阵、海瑟矩阵、海塞矩阵等，是一个多元函数的<strong>二阶偏导数</strong>构成的方阵，描述了函数的局部曲率。黑塞矩阵最早于19世纪由德国数学家Ludwig Otto Hesse提出，并以其名字命名。黑塞矩阵常用于牛顿法解决优化问题，利用黑塞矩阵可判定多元函数的极值问题。   —— 百度百科</p>
</blockquote>
<p>在深度学习的相关领域，很少会计算标量函数的单个二阶导数。 相反，我们通常对Hessian矩阵的性质比较感兴趣。 如果我们有函数 $f:\mathbb{R}^n \to \mathbb{R}$，那么Hessian矩阵的大小是 $n\times n$。 在典型的深度学习应用中，$n$ 将是模型的参数数量，可能很容易达到数十亿。 因此，完整的Hessian矩阵甚至不能表示。</p>
<p>典型的深度学习方法是使用Krylov方法，而不是显式地计算Hessian矩阵。 Krylov方法是用于执行各种操作的一组迭代技术，这些操作包括像近似求解矩阵的逆、或者近似矩阵的特征值或特征向量等，而不使用矩阵-向量乘法以外的任何操作。</p>
<p>为了在Hesssian矩阵上使用Krylov方法，我们只需要能够计算Hessian矩阵 $H$ 和一个任意向量 $v$ 间的乘积即可（该表达式中两个梯度的计算都可以由适当的软件库自动完成）：<br>$$ H v=\nabla_{x} \left [ (\nabla_{x} f(x))^\top v\right ] $$</p>
<p>虽然计算Hessian通常是不可取的，但是可以使用Hessian向量积。 可以对所有的 $i=1,\ldots,n$ 简单地计算 $H e^{(i)}$，其中 $e^{(i)}$ 是 $e_i^{(i)}=1$ 并且其他元素都为 $0$ 的 <strong>one-hot</strong> 向量（通过阅读<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py">源码</a>，我们发现Hessian向量积 $Hv$ 尚未实现成avaliable的状态，Tensorflow当前版本当前仅对Hesssian矩阵完成了实现）。</p>
<h2 id="其它：PyTorch的自动求导"><a href="#其它：PyTorch的自动求导" class="headerlink" title="其它：PyTorch的自动求导"></a>其它：PyTorch的自动求导</h2><p>PyTorch提供了包<code>torch.autograd</code>用于自动求导。在前向过程中，PyTorch会构建计算图，每个<strong>节点</strong>用Variable表示，<strong>边</strong>表示由输入节点到输出节点的函数（<code>torch.autograd.Function</code>对象）。Function对象不仅负责执行前向计算，在反向过程中，每个Function对象会调用<code>.backward()</code>函数计算输出对输入的梯度，然后将梯度传递给下一个Function对象。</p>
<h3 id="How-autograd-encodes-the-history-PyTorch"><a href="#How-autograd-encodes-the-history-PyTorch" class="headerlink" title="How autograd encodes the history (PyTorch)"></a>How autograd encodes the history (PyTorch)</h3><blockquote>
<p><strong>原文</strong> Internally, autograd represents this graph as a graph of <code>Function</code> objects (really expressions), which can be <code>apply()</code> ed to compute the result of evaluating the graph. When computing the forwards pass, autograd <strong>simultaneously performs</strong> the requested computations and builds up <strong>a graph representing the function that computes the gradient</strong> (the <code>.grad_fn</code> attribute of each <code>Variable</code> is <strong>an entry point</strong> into this graph). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients.</p>
</blockquote>
<blockquote>
<p>An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.</p>
</blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://pytorch.org/docs/master/notes/autograd.html#how-autograd-encodes-the-history">http://pytorch.org/docs/master/notes/autograd.html#how-autograd-encodes-the-history</a></p>
</blockquote>
</blockquote>
<p><strong>自动求导如何编码历史信息</strong> in <a target="_blank" rel="noopener" href="https://github.com/awfssv/pytorch-cn/blob/master/docs/notes/autograd.md">PyTorch-CN</a><br>每个变量都有一个<code>.creator</code>属性，它指向把它作为输出的函数。这是一个由<code>Function</code>对象作为节点组成的有向无环图（DAG）的入口点，它们之间的引用就是图的<strong>边</strong>。每次执行一个操作时，一个表示它的新<code>Function</code>就被实例化，它的<code>forward()</code>方法被调用，并且它输出的<code>Variable</code>的创建者被设置为这个<code>Function</code>。然后，通过跟踪从任何变量到叶节点的路径，可以重建创建数据的操作序列，并自动计算梯度。</p>
<p>需要注意的一点是，整个图在每次迭代时都是从头开始重新创建的，这就允许使用任意的Python控制流语句，这样可以在每次迭代时改变图的整体形状和大小。在启动训练之前不必对所有可能的路径进行编码—— <strong>what you run is what you differentiate</strong>.</p>
<h3 id="PyTorch中定义一个新操作"><a href="#PyTorch中定义一个新操作" class="headerlink" title="PyTorch中定义一个新操作"></a>PyTorch中定义一个新操作</h3><p>定义新的操作，意味着定义Function的子类，并且这些子类必须重写以下函数：<code>::forward()</code>和<code>::backward()</code>。初始化函数<code>::__init__()</code>根据实际需求判断是否需要重写。</p>
<blockquote>
<p><strong>forward()</strong><br>forward()可以有任意多个输入、任意多个输出，但是输入和输出必须是Variable。</p>
<p><strong>backward()</strong><br>backward()的输入和输出的个数就是forward()函数的输出和输入的个数。其中，backward()输入表示关于forward()输出的梯度，backward()的输出表示关于forward()的输入的梯度。在输入不需要梯度时（通过查看<code>needs_input_grad</code>参数）或者不可导时，可以返回None。</p>
<p><strong>Reference:</strong>  <a target="_blank" rel="noopener" href="http://blog.csdn.net/victoriaw/article/details/72566249">http://blog.csdn.net/victoriaw/article/details/72566249</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inherit from Function</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bias is an optional argument</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.save_for_backward(<span class="built_in">input</span>, weight, bias)</span><br><span class="line">        output = <span class="built_in">input</span>.mm(weight.t())</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output += bias.unsqueeze(<span class="number">0</span>).expand_as(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This function has only a single output, so it gets only one gradient</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, grad_output</span>):</span></span><br><span class="line">        <span class="comment"># This is a pattern that is very convenient - at the top of backward</span></span><br><span class="line">        <span class="comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span></span><br><span class="line">        <span class="comment"># None. Thanks to the fact that additional trailing Nones are</span></span><br><span class="line">        <span class="comment"># ignored, the return statement is simple even when the function has</span></span><br><span class="line">        <span class="comment"># optional inputs.</span></span><br><span class="line">        <span class="built_in">input</span>, weight, bias = self.saved_tensors</span><br><span class="line">        grad_input = grad_weight = grad_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># These needs_input_grad checks are optional and there only to</span></span><br><span class="line">        <span class="comment"># improve efficiency. If you want to make your code simpler, you can</span></span><br><span class="line">        <span class="comment"># skip them. Returning gradients for inputs that don&#x27;t require it is</span></span><br><span class="line">        <span class="comment"># not an error.</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input = grad_output.mm(weight)</span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_weight = grad_output.t().mm(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> self.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            grad_bias = grad_output.<span class="built_in">sum</span>(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input, grad_weight, grad_bias</span><br><span class="line"></span><br><span class="line"><span class="comment">#建议把新操作封装在一个函数中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span>(<span class="params"><span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># First braces create a Function object. Any arguments given here</span></span><br><span class="line">    <span class="comment"># will be passed to __init__. Second braces will invoke the __call__</span></span><br><span class="line">    <span class="comment"># operator, that will then use forward() to compute the result and</span></span><br><span class="line">    <span class="comment"># return it.</span></span><br><span class="line">    <span class="keyword">return</span> Linear()(<span class="built_in">input</span>, weight, bias)<span class="comment">#调用forward()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#检查实现的backward()是否正确</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> gradcheck</span><br><span class="line"><span class="comment"># gradchek takes a tuple of tensor as input, check if your gradient</span></span><br><span class="line"><span class="comment"># evaluated with these tensors are close enough to numerical</span></span><br><span class="line"><span class="comment"># approximations and returns True if they all verify this condition.</span></span><br><span class="line"><span class="built_in">input</span> = (Variable(torch.randn(<span class="number">20</span>,<span class="number">20</span>).double(), requires_grad=<span class="literal">True</span>),)</span><br><span class="line">test = gradcheck(Linear(), <span class="built_in">input</span>, eps=<span class="number">1e-6</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(test)</span><br></pre></td></tr></table></figure>
    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/Backpropagation/">#Backpropagation</a> <a href="/tags/Autograd/">#Autograd</a>
        </div>
    

    <!-- Comments -->
    
    <div class="comments">
        
<div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>



    </div>
    

</div>
        </section>

    </div>
</div>

</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    Coding the world, try to fetch more.
                </p>
                <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mihyim0z76&amp;m=7&amp;c=66ccff&amp;cr1=ff0000&amp;f=arial&amp;l=0&amp;lx=200&amp;ly=-20&amp;hi=10" async="async"></script>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/articles/2022-08/%E3%80%90SQLite3%E3%80%91%E4%BD%BF%E7%94%A8%20SQLite%20%E4%B8%BA%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C%20dataloader%20%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98.html">【SQLite3】使用 SQLite 为模型训练中的并行 dataloader </a>
            </li>
            
            <li>
                <a class="footer-post" href="/articles/2022-08/%E3%80%90SymbolicLink%E3%80%91%E5%88%A9%E7%94%A8%E8%BD%AF%E8%BF%9E%E6%8E%A5%E5%B0%86%E5%B7%B2%E5%AE%89%E8%A3%85%E7%A8%8B%E5%BA%8F%E6%90%AC%E8%BF%81%E5%88%B0%E5%85%B6%E4%BB%96%E7%9B%98%E7%AC%A6.html">【SymbolicLink】利用软连接将已安装程序迁到其他盘符</a>
            </li>
            
            <li>
                <a class="footer-post" href="/articles/2020-10/%E3%80%90Requests%E3%80%91Bilibili%20Security%201024%E7%A8%8B%E5%BA%8F%E8%8A%82%20CTF%E8%AE%B0%E5%BD%95.html">【Requests】Bilibili Security 1024程序节 CTF记</a>
            </li>
            
            <li>
                <a class="footer-post" href="/articles/2020-09/%E3%80%90CheatEngine%E3%80%91%E5%85%B3%E4%BA%8EBCR%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90.html">【CheatEngine】关于BCR的内存分析</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/">解题报告</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/%E6%97%A5%E5%B8%B8%E8%AE%B0%E5%BD%95/">日常记录</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BF%86/">编程记忆</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">环境配置</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a target="_blank" rel="noopener" href="https://github.com/okcd00/">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a target="_blank" rel="noopener" href="https://twitter.com/okcd00/">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a target="_blank" rel="noopener" href="http://okcd00.oschina.io/">
                            <span class="footer-icon-container">
                                <i class="fa fa-star-half-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:okcd00@vip.qq.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a target="_blank" rel="noopener" href="http://blog.csdn.net/okcd00">
                            <span class="footer-icon-container">
                                <i class="fa fa-pencil-square-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @<a target="_blank" rel="noopener" href="https://github.com/okcd00/">Dian Chen</a>. 2017-2022 All right reserved | Re-design & Deploy: <a href="blog.csdn.net/okcd00">okcd00</a> | Hexo Theme: <a target="_blank" rel="noopener" href="http://www.codeblocq.com/">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'okcd00';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<!-- "//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js" -->
<script type="text/javascript" color="0,255,255" opacity='0.75' zIndex="-2" count="75" src='/js/canvas.js'></script>

</body>
</html>